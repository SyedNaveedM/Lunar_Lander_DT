{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander Landing with Decision Transformer\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project demonstrates the application of a **Decision Transformer (DT)** model to solve the Lunar Lander environment from Gymnasium. The Decision Transformer is a novel architecture in reinforcement learning that frames the problem of optimal control as a sequence modeling task, leveraging the power of transformer networks, widely used in natural language processing. Instead of predicting actions based on the current state, the DT predicts actions conditioned on desired future returns, states, and actions, effectively acting as a goal-conditioned policy.\n",
    "\n",
    "The primary goal is to train an agent to successfully land the Lunar Lander module on a designated landing pad between two flags. This involves precise control over thelander's thrusters to manage its descent rate and horizontal movement.\n",
    "\n",
    "### Key Features:\n",
    "-   **Decision Transformer Implementation:** A custom PyTorch implementation of the Decision Transformer architecture.\n",
    "-   **Expert Trajectory Learning:** The model is trained on a dataset of expert trajectories, allowing it to learn optimal behaviors through imitation learning.\n",
    "-   **Optimized Training:** Incorporates several optimizations for efficient and stable training, including:\n",
    "    -   State normalization.\n",
    "    -   Optimized trajectory processing and sampling strategy.\n",
    "    -   Mixed precision training for faster computation.\n",
    "    -   Gradient clipping and learning rate scheduling.\n",
    "    -   Early stopping mechanism.\n",
    "-   **Pygame Visualization:** Real-time visualization of the agent's performance in the Lunar Lander environment during evaluation.\n",
    "\n",
    "### Environment: LunarLander-v3\n",
    "\n",
    "The Lunar Lander environment is a classic control problem in reinforcement learning. The agent controls a lander that needs to land safely between two flags on a landing pad. The state space consists of 8 continuous values (position, velocity, angle, angular velocity of the lander, and whether each leg is in contact with the ground). The action space consists of 4 discrete actions (do nothing, fire main engine, fire left engine, fire right engine).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "This section imports all necessary libraries and defines the global configuration parameters for the environment, model, training process, and evaluation. These parameters can be easily adjusted to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import collections\n",
    "from collections import deque\n",
    "import pygame\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "ENV_ID = 'LunarLander-v3'\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Trajectory Data\n",
    "TRAJECTORY_FILE = '../datasets/trajectories_5000.pkl' # Path to expert trajectories\n",
    "MIN_EXPERT_RETURN = 250 # Minimum return for a trajectory to be considered 'expert'\n",
    "\n",
    "# Optimized Decision Transformer Hyperparameters\n",
    "DT_CONTEXT_LEN = 20 # Length of the sequence context for the transformer\n",
    "DT_N_HEADS = 8 # Number of attention heads in the transformer\n",
    "DT_N_LAYERS = 6 # Number of transformer encoder layers\n",
    "DT_EMBED_DIM = 256 # Dimension of the embeddings\n",
    "DT_DROPOUT = 0.1 # Dropout rate for regularization\n",
    "DT_LR = 3e-4 # Learning rate for the optimizer\n",
    "DT_WEIGHT_DECAY = 1e-4 # Weight decay for L2 regularization\n",
    "DT_BATCH_SIZE = 256 # Batch size for training\n",
    "DT_NUM_EPOCHS = 50 # Number of training epochs\n",
    "DT_TRAJECTORIES_TO_USE = 5000 # Number of expert trajectories to use for training\n",
    "\n",
    "# Enhanced Early Stopping\n",
    "EARLY_STOPPING_PATIENCE = 5 # Number of epochs to wait for improvement before stopping\n",
    "EARLY_STOPPING_MIN_DELTA = 0.001 # Minimum change in loss to qualify as an improvement\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "USE_LR_SCHEDULER = True # Whether to use a learning rate scheduler\n",
    "SCHEDULER_FACTOR = 0.8 # Factor by which the learning rate will be reduced\n",
    "SCHEDULER_PATIENCE = 3 # Number of epochs with no improvement after which learning rate will be reduced\n",
    "\n",
    "# Data Augmentation\n",
    "USE_DATA_AUGMENTATION = True # Whether to apply data augmentation to states\n",
    "AUGMENTATION_NOISE_STD = 0.01 # Standard deviation of noise for data augmentation\n",
    "\n",
    "# Pygame Visualization\n",
    "PYGAME_FPS = 60 # Frames per second for Pygame visualization\n",
    "NUM_EVAL_EPISODES = 10 # Number of episodes to run for evaluation\n",
    "\n",
    "# Global Normalization Parameters (will be calculated from data)\n",
    "state_mean = None\n",
    "state_std = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "\n",
    "This section defines helper functions essential for data preparation and overall system stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Sets the random seed for reproducibility across different libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def load_trajectories_from_pickle(file_path):\n",
    "    \"\"\"Loads expert trajectories from a pickle file.\"\"\"\n",
    "    print(f\"\\n--- Loading Expert Trajectories from {file_path} ---\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: Trajectory file not found at {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            trajectories = pickle.load(f)\n",
    "        print(f\"Successfully loaded {len(trajectories)} trajectories.\")\n",
    "        return trajectories\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading trajectories from pickle file: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_state_normalization_params(trajectories):\n",
    "    \"\"\"Calculates mean and standard deviation for state normalization across all trajectories.\n",
    "    Normalization is crucial for neural networks to ensure stable training by centering\n",
    "    inputs around zero mean and scaling them to similar magnitudes. This prevents issues\n",
    "    like poor gradient flow, slow convergence, and overfitting to high-magnitude features.\n",
    "    \"\"\"\n",
    "    print(\"Calculating state normalization parameters...\")\n",
    "    \n",
    "    all_states = []\n",
    "    for traj in trajectories:\n",
    "        all_states.append(np.array(traj['states']))\n",
    "    \n",
    "    all_states = np.concatenate(all_states, axis=0).astype(np.float32)\n",
    "    \n",
    "    mean = np.mean(all_states, axis=0, keepdims=True)\n",
    "    std = np.std(all_states, axis=0, keepdims=True)\n",
    "    std = np.maximum(std, 1e-8)  # Prevent division by zero\n",
    "    \n",
    "    print(f\"Processed {len(all_states)} states for normalization\")\n",
    "    return mean.flatten(), std.flatten()\n",
    "\n",
    "def normalize_state(state, mean, std):\n",
    "    \"\"\"Normalizes a single state vector using the pre-calculated mean and standard deviation.\"\"\"\n",
    "    return (state - mean) / std\n",
    "\n",
    "def process_trajectories_optimized(raw_trajectories, context_len, gamma=0.995, state_mean=None, state_std=None):\n",
    "    \"\"\"Processes raw trajectories into a format suitable for Decision Transformer training.\n",
    "    This includes normalizing states, calculating returns-to-go, and extracting sequences\n",
    "    of a specified context length. It also implements a smart sampling strategy to prioritize\n",
    "    high-value segments of trajectories.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    print(\"Processing trajectories with optimized sampling...\")\n",
    "    \n",
    "    for trajectory in tqdm(raw_trajectories, desc=\"Processing trajectories\"):\n",
    "        states = np.array(trajectory['states'], dtype=np.float32)\n",
    "        actions = np.array(trajectory['actions'], dtype=np.int64)\n",
    "        rewards = np.array(trajectory['rewards'], dtype=np.float32)\n",
    "        \n",
    "        # Vectorized normalization\n",
    "        normalized_states = (states - state_mean) / state_std\n",
    "        \n",
    "        # Vectorized returns-to-go calculation: G_t = R_t + gamma * G_{t+1}\n",
    "        returns_to_go = np.zeros_like(rewards)\n",
    "        returns_to_go[-1] = rewards[-1]\n",
    "        for t in range(len(rewards) - 2, -1, -1):\n",
    "            returns_to_go[t] = rewards[t] + gamma * returns_to_go[t + 1]\n",
    "        \n",
    "        traj_len = len(normalized_states)\n",
    "        \n",
    "        # Smart sampling: sample more from high-value parts of trajectory\n",
    "        # This helps the model focus on more critical and informative segments\n",
    "        high_value_threshold = np.percentile(returns_to_go, 70)\n",
    "        high_value_indices = np.where(returns_to_go >= high_value_threshold)[0]\n",
    "        \n",
    "        # Base sampling evenly distributes samples across the trajectory\n",
    "        sample_indices = []\n",
    "        for i in range(0, traj_len, max(1, traj_len // 20)): \n",
    "            sample_indices.append(i)\n",
    "        \n",
    "        # Add extra samples from high-value states for more focused learning\n",
    "        for idx in high_value_indices[::2]: # Every other high-value state\n",
    "            if idx not in sample_indices:\n",
    "                sample_indices.append(idx)\n",
    "        \n",
    "        sample_indices = sorted(set(sample_indices))\n",
    "        \n",
    "        for i in sample_indices:\n",
    "            end_idx = i + 1\n",
    "            start_idx = max(0, end_idx - context_len)\n",
    "            \n",
    "            # Extract sequences for states, actions, returns-to-go, and timesteps\n",
    "            s_seq = normalized_states[start_idx:end_idx]\n",
    "            a_seq = actions[start_idx:end_idx]\n",
    "            r_seq = returns_to_go[start_idx:end_idx]\n",
    "            timesteps_seq = np.arange(start_idx, end_idx)\n",
    "            \n",
    "            # Pad sequences to `context_len` if they are shorter\n",
    "            # Padding ensures all sequences have a consistent length for batching\n",
    "            pad_len = context_len - len(s_seq)\n",
    "            if pad_len > 0:\n",
    "                s_seq = np.vstack([np.zeros((pad_len, s_seq.shape[1])), s_seq])\n",
    "                a_seq = np.concatenate([np.zeros(pad_len, dtype=np.int64), a_seq])\n",
    "                r_seq = np.concatenate([np.zeros(pad_len), r_seq])\n",
    "                # Timesteps padding should be relative to the sequence start\n",
    "                timesteps_seq = np.concatenate([np.zeros(pad_len, dtype=np.int64), np.arange(len(timesteps_seq)) + pad_len])\n",
    "            \n",
    "            processed_data.append({\n",
    "                'states': s_seq.astype(np.float32),\n",
    "                'actions': a_seq.astype(np.int64),\n",
    "                'returns_to_go': r_seq.astype(np.float32),\n",
    "                'timesteps': timesteps_seq.astype(np.int64),\n",
    "                'target_action': actions[i] # The action taken at the current timestep 'i'\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated {len(processed_data)} training samples\")\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "The `OptimizedTrajectoryDataset` class is a PyTorch `Dataset` that prepares the processed trajectory data for the DataLoader. It handles fetching individual samples and applies data augmentation if configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedTrajectoryDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Decision Transformer training.\n",
    "    It provides sequences of states, actions, returns-to-go, and timesteps.\n",
    "    Supports optional data augmentation by adding noise to states.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, use_augmentation=False, noise_std=0.01):\n",
    "        self.data = data\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        states = item['states'].copy()\n",
    "        \n",
    "        # Data augmentation: add small noise to states to improve robustness\n",
    "        if self.use_augmentation and random.random() < 0.3: # Apply noise with 30% probability\n",
    "            noise = np.random.normal(0, self.noise_std, states.shape).astype(np.float32)\n",
    "            states = states + noise\n",
    "        \n",
    "        return (torch.tensor(states, dtype=torch.float32),\n",
    "                torch.tensor(item['actions'], dtype=torch.long),\n",
    "                torch.tensor(item['returns_to_go'], dtype=torch.float32),\n",
    "                torch.tensor(item['timesteps'], dtype=torch.long),\n",
    "                torch.tensor(item['target_action'], dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Transformer Model (`OptimizedDecisionTransformer`)\n",
    "\n",
    "This is the core of the project: a PyTorch implementation of the Decision Transformer. The model takes sequences of states, actions, returns-to-go, and timesteps as input and predicts the next action. It leverages a transformer encoder for sequence modeling and uses various optimizations for better performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDecisionTransformer(nn.Module):\n",
    "    \"\"\"Optimized Decision Transformer model for sequence modeling in reinforcement learning.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, embed_dim, context_len, n_heads, n_layers, dropout, max_timestep=4096):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "\n",
    "        # State embedding: Projects raw state vector into the embedding dimension\n",
    "        self.state_embedding = nn.Sequential(\n",
    "            nn.Linear(state_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5) # Reduced dropout for embedding\n",
    "        )\n",
    "        \n",
    "        # Action embedding: Uses nn.Embedding for discrete actions, which is efficient for categorical inputs\n",
    "        self.action_embedding = nn.Embedding(action_dim, embed_dim)\n",
    "\n",
    "        # Return-to-go embedding: Projects scalar return-to-go into the embedding dimension\n",
    "        self.return_embedding = nn.Sequential(\n",
    "            nn.Linear(1, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Learnable positional embedding: Adds positional information to the sequence tokens.\n",
    "        # The length is `context_len * 3` because each timestep has 3 tokens: (return, state, action).\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, context_len * 3, embed_dim) * 0.02)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer Encoder Layer: Building block of the transformer.\n",
    "        # `norm_first=False` (Post-norm) is used here as it can sometimes be more stable with specific initializations\n",
    "        # or if pre-norm introduces too much initial noise.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=2 * embed_dim, # Reduced from 4x for efficiency\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=False \n",
    "        )\n",
    "\n",
    "        # Transformer Encoder: Stacks multiple `encoder_layer` instances.\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Action prediction head: Maps the transformer's output to action probabilities.\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better training stability\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initializes weights of linear layers, embeddings, and normalization layers for optimal training.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        \"\"\"Forward pass of the Decision Transformer.\n",
    "        Processes embedded states, actions, and returns-to-go through the transformer.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "        \n",
    "        # Embed each modality\n",
    "        state_embeddings = self.state_embedding(states)\n",
    "        action_embeddings = self.action_embedding(actions)\n",
    "        # Unsqueeze adds a dimension (batch_size, seq_len, 1) to match linear layer input\n",
    "        return_embeddings = self.return_embedding(returns_to_go.unsqueeze(-1))\n",
    "        \n",
    "        # Stack tokens efficiently as (batch_size, seq_len, 3, embed_dim)\n",
    "        # and then reshape to (batch_size, seq_len * 3, embed_dim).\n",
    "        # The order is crucial: (return, state, action) for causal masking.\n",
    "        stacked_inputs = torch.stack([return_embeddings, state_embeddings, action_embeddings], dim=2)\n",
    "        stacked_inputs = stacked_inputs.reshape(batch_size, seq_len * 3, self.embed_dim)\n",
    "        \n",
    "        # Add learnable positional embeddings to the stacked inputs.\n",
    "        stacked_inputs = stacked_inputs + self.pos_embedding[:, :seq_len * 3, :]\n",
    "        stacked_inputs = self.dropout(stacked_inputs)\n",
    "        \n",
    "        # Optimized causal mask to prevent attention to future tokens. \n",
    "        # For a given timestep `t`, the action token `a_t` should only attend to `s_t`, `r_t`, and `s_<t`, `a_<t`, `r_<t`.\n",
    "        mask = self._create_optimized_causal_mask(seq_len, stacked_inputs.device)\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        transformer_outputs = self.transformer(stacked_inputs, mask=mask)\n",
    "        \n",
    "        # Extract state tokens: these are at indices 1, 4, 7, ... (1 + 3*k)\n",
    "        # The model predicts actions based on the state representations.\n",
    "        state_tokens = transformer_outputs[:, 1::3, :]\n",
    "        \n",
    "        # Predict actions using the action head\n",
    "        action_preds = self.action_head(state_tokens)\n",
    "        \n",
    "        return action_preds\n",
    "\n",
    "    def _create_optimized_causal_mask(self, seq_len, device):\n",
    "        \"\"\"Creates an optimized causal mask for the Decision Transformer.\n",
    "        This mask ensures that the transformer adheres to the causal structure of RL sequences,\n",
    "        where predictions for time `t` can only depend on information up to time `t`.\n",
    "        The mask prevents tokens from attending to future states, actions, or returns.\n",
    "        \"\"\"\n",
    "        total_len = seq_len * 3 # Total tokens in the sequence (return, state, action per timestep)\n",
    "        \n",
    "        # Initialize a base causal mask (upper triangular matrix with -inf)\n",
    "        mask = torch.triu(torch.ones(total_len, total_len, device=device) * float('-inf'), diagonal=1)\n",
    "        \n",
    "        # Adjust mask for Decision Transformer token structure (R_t, S_t, A_t)\n",
    "        for i in range(total_len):\n",
    "            timestep = i // 3 # Current timestep index\n",
    "            token_type = i % 3  # 0 for Return, 1 for State, 2 for Action\n",
    "            \n",
    "            # A token can always see all past tokens.\n",
    "            # Specific causal adjustments for current timestep:\n",
    "            if token_type == 1:  # State token (S_t)\n",
    "                # S_t can see R_t (its corresponding return-to-go for current timestep)\n",
    "                mask[i, timestep * 3] = 0 \n",
    "            elif token_type == 2:  # Action token (A_t)\n",
    "                # A_t can see R_t and S_t (its corresponding return-to-go and state)\n",
    "                mask[i, timestep * 3:timestep * 3 + 2] = 0\n",
    "        \n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function (`train_optimized_decision_transformer`)\n",
    "\n",
    "This function orchestrates the training loop for the Decision Transformer. It includes:\n",
    "-   **Mixed Precision Training:** Utilizes `torch.amp.GradScaler` for faster training and reduced memory usage on compatible hardware (GPUs).\n",
    "-   **Gradient Clipping:** Prevents exploding gradients by limiting their norm.\n",
    "-   **Early Stopping:** Monitors the validation loss and stops training if no significant improvement is observed for a certain number of epochs, preventing overfitting.\n",
    "-   **Learning Rate Scheduling:** Adjusts the learning rate dynamically based on the training loss, helping the model converge more effectively.\n",
    "-   **Model Saving:** Saves the best performing model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimized_decision_transformer(model, dataloader, optimizer, scheduler, num_epochs, patience, min_delta):\n",
    "    \"\"\"Trains the Decision Transformer model with optimizations.\n",
    "    Includes mixed precision training, gradient clipping, early stopping, and learning rate scheduling.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training Optimized Decision Transformer ---\")\n",
    "    model.train() # Set model to training mode\n",
    "    \n",
    "    # Initialize early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Mixed precision training for faster computation on CUDA devices\n",
    "    scaler = torch.amp.GradScaler('cuda') if DEVICE.type == 'cuda' else None\n",
    "\n",
    "    # Path to save the best model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_path = '../models/lunar_lander_model_5000.pkl'\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        for states, actions, returns_to_go, timesteps, target_actions in progress_bar:\n",
    "            # Move data to the appropriate device (CPU/GPU)\n",
    "            states = states.to(DEVICE, non_blocking=True)\n",
    "            actions = actions.to(DEVICE, non_blocking=True)\n",
    "            returns_to_go = returns_to_go.to(DEVICE, non_blocking=True)\n",
    "            timesteps = timesteps.to(DEVICE, non_blocking=True)\n",
    "            target_actions = target_actions.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients from previous step\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if scaler is not None:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    action_preds = model(states, actions, returns_to_go, timesteps)\n",
    "                    # Calculate Cross-Entropy Loss for action prediction (only for the last action in sequence)\n",
    "                    loss = F.cross_entropy(action_preds[:, -1, :], target_actions)\n",
    "                \n",
    "                scaler.scale(loss).backward() # Scale loss and perform backward pass\n",
    "                scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # Clip gradients\n",
    "                scaler.step(optimizer) # Update model parameters\n",
    "                scaler.update() # Update the scaler for the next iteration\n",
    "            else:\n",
    "                # Standard float32 training\n",
    "                action_preds = model(states, actions, returns_to_go, timesteps)\n",
    "                loss = F.cross_entropy(action_preds[:, -1, :], target_actions)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy (no gradient calculation needed)\n",
    "            with torch.no_grad():\n",
    "                predicted_actions = torch.argmax(action_preds[:, -1, :], dim=-1)\n",
    "                accuracy = (predicted_actions == target_actions).float().mean()\n",
    "            \n",
    "            # Aggregate loss and accuracy for epoch statistics\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar with current batch metrics\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{total_loss/num_batches:.4f}', \n",
    "                'Acc': f'{total_accuracy/num_batches:.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_accuracy = total_accuracy / num_batches\n",
    "        \n",
    "        # Step the learning rate scheduler if enabled\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Loss={avg_loss:.4f}, Acc={avg_accuracy:.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the model's state dictionary if it's the best so far\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            torch.save({\n",
    "                'model_state_dict': best_model_state,\n",
    "                'state_mean': state_mean,\n",
    "                'state_std': state_std,\n",
    "                'model_config': {\n",
    "                    'state_dim': model.state_dim,\n",
    "                    'action_dim': model.action_dim,\n",
    "                    'embed_dim': model.embed_dim,\n",
    "                    'context_len': model.context_len,\n",
    "                    'n_heads': DT_N_HEADS,\n",
    "                    'n_layers': DT_N_LAYERS,\n",
    "                    'dropout': DT_DROPOUT\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"✓ New best model saved with loss: {best_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\n🛑 Early stopping after {epoch + 1} epochs!\")\n",
    "                break\n",
    "\n",
    "    # Load the best model state after training (if early stopping occurred)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"✓ Model restored to best state (loss: {best_loss:.4f})\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pygame Visualization Functions\n",
    "\n",
    "These functions handle the setup and rendering of the Lunar Lander environment using Pygame, allowing for real-time visualization of the agent's performance during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pygame(width, height):\n",
    "    \"\"\"Initializes Pygame for rendering the environment.\"\"\"\n",
    "    pygame.init()\n",
    "    # Add extra height for displaying text (score, episode number)\n",
    "    screen = pygame.display.set_mode((width, height + 50))\n",
    "    pygame.display.set_caption(\"Optimized Lunar Lander Decision Transformer\")\n",
    "    font = pygame.font.Font(None, 36) # Default font, size 36\n",
    "    return screen, font\n",
    "\n",
    "def render_env_pygame(env, screen, font, current_score, episode_num, total_episodes):\n",
    "    \"\"\"Renders the Gymnasium environment using Pygame and displays score/episode info.\"\"\"\n",
    "    frame = env.render() # Get the RGB array frame from the environment\n",
    "    if frame is not None:\n",
    "        # Pygame expects (width, height, channels), so transpose the numpy array\n",
    "        frame = np.transpose(frame, (1, 0, 2))\n",
    "        pygame_surface = pygame.surfarray.make_surface(frame)\n",
    "        screen.fill((0, 0, 0)) # Clear screen with black\n",
    "        screen.blit(pygame_surface, (0, 0)) # Draw the environment frame\n",
    "    \n",
    "    # Render score and episode information\n",
    "    score_text = font.render(f\"Score: {current_score:.1f}\", True, (255, 255, 255)) # White color\n",
    "    episode_text = font.render(f\"Episode: {episode_num}/{total_episodes}\", True, (255, 255, 255))\n",
    "    \n",
    "    screen.blit(score_text, (10, screen.get_height() - 40)) # Position at bottom-left\n",
    "    screen.blit(episode_text, (10, screen.get_height() - 80)) # Position above score\n",
    "    pygame.display.flip() # Update the full display Surface to the screen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Execution Block\n",
    "\n",
    "This block orchestrates the entire workflow: environment setup, data loading and preprocessing, model training, and evaluation with visualization. This is where all the previously defined functions are called in sequence to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"🚀 Device: {DEVICE}\")\n",
    "    print(f\"🎯 Using up to {DT_TRAJECTORIES_TO_USE} trajectories with return >= {MIN_EXPERT_RETURN}\")\n",
    "\n",
    "    # --- 7.1 Environment Initialization ---\n",
    "    env_dt = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "    state_dim = env_dt.observation_space.shape[0]\n",
    "    action_dim = env_dt.action_space.n\n",
    "    # Get max_episode_steps if available, otherwise default to 1000\n",
    "    max_timestep = env_dt.spec.max_episode_steps if env_dt.spec else 1000\n",
    "\n",
    "    # --- 7.2 Load and Filter Trajectories ---\n",
    "    raw_expert_trajectories = load_trajectories_from_pickle(TRAJECTORY_FILE)\n",
    "    if not raw_expert_trajectories:\n",
    "        print(\"❌ No trajectories loaded. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Smart filtering: prioritize high-performing trajectories for training\n",
    "    # Sort trajectories by their total return in descending order.\n",
    "    trajectory_returns = [(i, sum(traj['rewards'])) for i, traj in enumerate(raw_expert_trajectories)]\n",
    "    trajectory_returns.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select a proportion of the top-performing trajectories\n",
    "    top_indices = [idx for idx, ret in trajectory_returns[:int(DT_TRAJECTORIES_TO_USE * 0.8)] if ret >= MIN_EXPERT_RETURN]\n",
    "    # Select some random remaining good trajectories for diversity\n",
    "    remaining_good = [idx for idx, ret in trajectory_returns[int(DT_TRAJECTORIES_TO_USE * 0.8):] if ret >= MIN_EXPERT_RETURN]\n",
    "    \n",
    "    random_indices = []\n",
    "    if len(remaining_good) > 0:\n",
    "        random_indices = random.sample(remaining_good, min(len(remaining_good), DT_TRAJECTORIES_TO_USE - len(top_indices)))\n",
    "    \n",
    "    selected_indices = top_indices + random_indices\n",
    "    # Final filtered list of trajectories, truncated to DT_TRAJECTORIES_TO_USE\n",
    "    filtered_trajectories = [raw_expert_trajectories[i] for i in selected_indices[:DT_TRAJECTORIES_TO_USE]]\n",
    "    \n",
    "    print(f\"✓ Selected {len(filtered_trajectories)} high-quality trajectories\")\n",
    "    print(f\"  📊 Mean return of selected trajectories: {np.mean([sum(traj['rewards']) for traj in filtered_trajectories]):.1f}\")\n",
    "\n",
    "    # --- 7.3 Data Preparation ---\n",
    "    # Calculate normalization parameters from the selected trajectories\n",
    "    state_mean, state_std = calculate_state_normalization_params(filtered_trajectories)\n",
    "\n",
    "    # Process trajectories for Decision Transformer input format\n",
    "    processed_dt_data = process_trajectories_optimized(\n",
    "        filtered_trajectories, DT_CONTEXT_LEN, gamma=0.995, \n",
    "        state_mean=state_mean, state_std=state_std\n",
    "    )\n",
    "    \n",
    "    # Create Dataset and DataLoader for efficient batching during training\n",
    "    dt_dataset = OptimizedTrajectoryDataset(processed_dt_data, USE_DATA_AUGMENTATION, AUGMENTATION_NOISE_STD)\n",
    "    dt_dataloader = DataLoader(\n",
    "        dt_dataset, \n",
    "        batch_size=DT_BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2, # Use multiple workers for faster data loading\n",
    "        pin_memory=True if DEVICE.type == 'cuda' else False # Pin memory for faster GPU transfers\n",
    "    )\n",
    "\n",
    "    # --- 7.4 Model Initialization ---\n",
    "    dt_model = OptimizedDecisionTransformer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        embed_dim=DT_EMBED_DIM,\n",
    "        context_len=DT_CONTEXT_LEN,\n",
    "        n_heads=DT_N_HEADS,\n",
    "        n_layers=DT_N_LAYERS,\n",
    "        dropout=DT_DROPOUT,\n",
    "        max_timestep=max_timestep\n",
    "    ).to(DEVICE) # Move model to the specified device\n",
    "\n",
    "    # Optimized AdamW optimizer with custom betas for transformers\n",
    "    dt_optimizer = optim.AdamW(\n",
    "        dt_model.parameters(), \n",
    "        lr=DT_LR, \n",
    "        weight_decay=DT_WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.95) \n",
    "    )\n",
    "    \n",
    "    scheduler = None\n",
    "    if USE_LR_SCHEDULER:\n",
    "        # ReduceLROnPlateau scheduler reduces LR when a metric has stopped improving\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            dt_optimizer, mode='min', factor=SCHEDULER_FACTOR, \n",
    "            patience=SCHEDULER_PATIENCE\n",
    "        )\n",
    "\n",
    "    print(f\"🔧 Model parameters: {sum(p.numel() for p in dt_model.parameters()):,}\")\n",
    "\n",
    "    # --- 7.5 Model Training ---\n",
    "    start_time = time.time()\n",
    "    dt_model = train_optimized_decision_transformer(\n",
    "        dt_model, dt_dataloader, dt_optimizer, scheduler, \n",
    "        DT_NUM_EPOCHS, EARLY_STOPPING_PATIENCE, EARLY_STOPPING_MIN_DELTA\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"⏱️  Training completed in {training_time:.1f} seconds\")\n",
    "\n",
    "    # --- 7.6 Evaluation and Visualization ---\n",
    "    print(\"\\n🎮 Starting evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        # Prompt user for number of evaluation episodes\n",
    "        NUM_EVAL_EPISODES = int(input(f\"Enter number of episodes to watch (default: {NUM_EVAL_EPISODES}): \") or NUM_EVAL_EPISODES)\n",
    "    except ValueError:\n",
    "        print(f\"Invalid input, using default {NUM_EVAL_EPISODES} episodes.\")\n",
    "\n",
    "    # Initialize pygame for rendering\n",
    "    _ = env_dt.reset() # Reset env once to get initial frame for dimension\n",
    "    dummy_frame = env_dt.render()\n",
    "    render_width = dummy_frame.shape[1] if dummy_frame is not None else 600\n",
    "    render_height = dummy_frame.shape[0] if dummy_frame is not None else 400\n",
    "\n",
    "    pygame_screen, pygame_font = init_pygame(render_width, render_height)\n",
    "    clock = pygame.time.Clock() # To control frame rate\n",
    "\n",
    "    dt_model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
    "    eval_returns = []\n",
    "    \n",
    "    for i_episode in range(NUM_EVAL_EPISODES):\n",
    "        state, _ = env_dt.reset() # Reset environment for new episode\n",
    "        state = normalize_state(state, state_mean, state_std)\n",
    "\n",
    "        # Initialize context deques with padding\n",
    "        states = deque(maxlen=DT_CONTEXT_LEN)\n",
    "        actions = deque(maxlen=DT_CONTEXT_LEN)\n",
    "        returns_to_go = deque(maxlen=DT_CONTEXT_LEN)\n",
    "        timesteps = deque(maxlen=DT_CONTEXT_LEN)\n",
    "\n",
    "        current_episode_return = 0\n",
    "        # Set a target return for the agent to aim for\n",
    "        # This is a crucial aspect of Decision Transformers\n",
    "        target_return = MIN_EXPERT_RETURN * 1.2 \n",
    "\n",
    "        # Pre-populate context with zeros/defaults for the initial steps\n",
    "        for _ in range(DT_CONTEXT_LEN):\n",
    "            states.append(np.zeros(state_dim, dtype=np.float32))\n",
    "            actions.append(0) # Default action\n",
    "            returns_to_go.append(0.0)\n",
    "            timesteps.append(0)\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference\n",
    "            for t in range(max_timestep):\n",
    "                # Update context with current state, estimated return, and timestep\n",
    "                states.append(state)\n",
    "                # The return-to-go is dynamic: it's the target return minus current cumulative reward\n",
    "                returns_to_go.append(max(target_return - current_episode_return, 0)) \n",
    "                timesteps.append(t)\n",
    "                \n",
    "                # Prepare inputs for the model (unsqueeze to add batch dimension)\n",
    "                s_input = torch.tensor(np.array(list(states)), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                a_input = torch.tensor(np.array(list(actions)), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "                r_input = torch.tensor(np.array(list(returns_to_go)), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                t_input = torch.tensor(np.array(list(timesteps)), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "                # Model inference: predict actions\n",
    "                action_preds = dt_model(s_input, a_input, r_input, t_input)\n",
    "                # Select the action with the highest probability from the last prediction in sequence\n",
    "                predicted_action = torch.argmax(action_preds[0, -1, :]).item()\n",
    "\n",
    "                # Take a step in the environment\n",
    "                next_state, reward, terminated, truncated, _ = env_dt.step(predicted_action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                next_state = normalize_state(next_state, state_mean, state_std)\n",
    "                current_episode_return += reward\n",
    "                state = next_state\n",
    "                actions.append(predicted_action) # Add taken action to context\n",
    "\n",
    "                # Render the environment using Pygame\n",
    "                render_env_pygame(env_dt, pygame_screen, pygame_font, current_episode_return, i_episode + 1, NUM_EVAL_EPISODES)\n",
    "                clock.tick(PYGAME_FPS) # Control rendering speed\n",
    "\n",
    "                # Handle Pygame events (e.g., closing the window)\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        exit()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        eval_returns.append(current_episode_return)\n",
    "        print(f\"Episode {i_episode + 1}: Return = {current_episode_return:.2f}\")\n",
    "\n",
    "    # --- 7.7 Evaluation Results Summary ---\n",
    "    print(f\"\\n📊 Evaluation Results:\")\n",
    "    print(f\"Mean Return: {np.mean(eval_returns):.2f} ± {np.std(eval_returns):.2f}\")\n",
    "    print(f\"Best Return: {np.max(eval_returns):.2f}\")\n",
    "    # Calculate success rate (e.g., return >= 200 is generally considered a success for Lunar Lander)\n",
    "    print(f\"Success Rate: {sum(1 for r in eval_returns if r >= 200)/len(eval_returns)*100:.1f}%\")\n",
    "    \n",
    "    env_dt.close() # Close the Gymnasium environment\n",
    "    pygame.quit() # Quit Pygame\n",
    "    print(\"✅ Evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
